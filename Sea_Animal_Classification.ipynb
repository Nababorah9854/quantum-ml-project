{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:54.252681Z",
     "iopub.status.busy": "2025-05-12T20:23:54.252280Z",
     "iopub.status.idle": "2025-05-12T20:23:55.514288Z",
     "shell.execute_reply": "2025-05-12T20:23:55.513380Z",
     "shell.execute_reply.started": "2025-05-12T20:23:54.252656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:55.515687Z",
     "iopub.status.busy": "2025-05-12T20:23:55.515391Z",
     "iopub.status.idle": "2025-05-12T20:23:55.519575Z",
     "shell.execute_reply": "2025-05-12T20:23:55.518860Z",
     "shell.execute_reply.started": "2025-05-12T20:23:55.515668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# General Utilities\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:55.520787Z",
     "iopub.status.busy": "2025-05-12T20:23:55.520520Z",
     "iopub.status.idle": "2025-05-12T20:23:55.535335Z",
     "shell.execute_reply": "2025-05-12T20:23:55.534665Z",
     "shell.execute_reply.started": "2025-05-12T20:23:55.520766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Numerical Computation & Visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:55.537520Z",
     "iopub.status.busy": "2025-05-12T20:23:55.537091Z",
     "iopub.status.idle": "2025-05-12T20:23:58.629495Z",
     "shell.execute_reply": "2025-05-12T20:23:58.628690Z",
     "shell.execute_reply.started": "2025-05-12T20:23:55.537498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install pennylane\n",
    "!pip install pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:58.630849Z",
     "iopub.status.busy": "2025-05-12T20:23:58.630606Z",
     "iopub.status.idle": "2025-05-12T20:23:58.634833Z",
     "shell.execute_reply": "2025-05-12T20:23:58.634034Z",
     "shell.execute_reply.started": "2025-05-12T20:23:58.630826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Quantum Machine Learning\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:58.636025Z",
     "iopub.status.busy": "2025-05-12T20:23:58.635772Z",
     "iopub.status.idle": "2025-05-12T20:23:58.658734Z",
     "shell.execute_reply": "2025-05-12T20:23:58.658115Z",
     "shell.execute_reply.started": "2025-05-12T20:23:58.636003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PyTorch: Deep Learning Framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:58.659679Z",
     "iopub.status.busy": "2025-05-12T20:23:58.659502Z",
     "iopub.status.idle": "2025-05-12T20:23:58.677434Z",
     "shell.execute_reply": "2025-05-12T20:23:58.676884Z",
     "shell.execute_reply.started": "2025-05-12T20:23:58.659666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Torchvision: Dataset & Model Utilities\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models\n",
    "from torchvision.models import (\n",
    "    ResNet50_Weights,\n",
    "    ResNet34_Weights,\n",
    "    ResNet101_Weights,\n",
    "    DenseNet121_Weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:58.678340Z",
     "iopub.status.busy": "2025-05-12T20:23:58.678166Z",
     "iopub.status.idle": "2025-05-12T20:23:58.694416Z",
     "shell.execute_reply": "2025-05-12T20:23:58.693912Z",
     "shell.execute_reply.started": "2025-05-12T20:23:58.678327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Scikit-learn: Evaluation Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:58.695216Z",
     "iopub.status.busy": "2025-05-12T20:23:58.695059Z",
     "iopub.status.idle": "2025-05-12T20:23:58.712451Z",
     "shell.execute_reply": "2025-05-12T20:23:58.711954Z",
     "shell.execute_reply.started": "2025-05-12T20:23:58.695204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "SEED = 42\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "N_QUBITS = 8\n",
    "LEARNING_RATE = 0.00002\n",
    "WEIGHT_DECAY = 5e-5\n",
    "LR_FACTOR = 0.2\n",
    "LR_PATIENCE = 8\n",
    "NUM_EPOCHS = 30\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.2\n",
    "UNDERSAMPLE_TURTLE_TORTOISE = True\n",
    "DATA_DIR = \"/kaggle/input/sea-animals-image-dataste\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:58.715321Z",
     "iopub.status.busy": "2025-05-12T20:23:58.715111Z",
     "iopub.status.idle": "2025-05-12T20:23:58.737273Z",
     "shell.execute_reply": "2025-05-12T20:23:58.736777Z",
     "shell.execute_reply.started": "2025-05-12T20:23:58.715307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "qml.numpy.random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "os.environ['NCCL_DEBUG'] = 'INFO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:58.738295Z",
     "iopub.status.busy": "2025-05-12T20:23:58.738059Z",
     "iopub.status.idle": "2025-05-12T20:23:58.756109Z",
     "shell.execute_reply": "2025-05-12T20:23:58.755374Z",
     "shell.execute_reply.started": "2025-05-12T20:23:58.738263Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💻 Using device: cuda (2 GPU(s))\n",
      "Setting default CUDA device to index 0.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"\\n💻 Using device: {device} ({num_gpus} GPU(s))\")\n",
    "    if num_gpus > 1:\n",
    "        torch.cuda.set_device(0)\n",
    "        print(f\"Setting default CUDA device to index 0.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    num_gpus = 0\n",
    "    print(f\"\\n💻 Using device: {device} (CPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:58.757146Z",
     "iopub.status.busy": "2025-05-12T20:23:58.756901Z",
     "iopub.status.idle": "2025-05-12T20:23:58.776427Z",
     "shell.execute_reply": "2025-05-12T20:23:58.775914Z",
     "shell.execute_reply.started": "2025-05-12T20:23:58.757125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        transforms.RandomRotation(degrees=8),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.03),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.02, 0.02), scale=(0.98, 1.02)),\n",
    "        transforms.RandomGrayscale(p=0.03),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:23:58.777604Z",
     "iopub.status.busy": "2025-05-12T20:23:58.777346Z",
     "iopub.status.idle": "2025-05-12T20:24:12.463808Z",
     "shell.execute_reply": "2025-05-12T20:24:12.462982Z",
     "shell.execute_reply.started": "2025-05-12T20:23:58.777584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform_initial = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])\n",
    "try:\n",
    "    full_dataset = datasets.ImageFolder(root=DATA_DIR, transform=transform_initial)\n",
    "    classes = full_dataset.classes\n",
    "    targets = np.array(full_dataset.targets)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    print(\"📂 Classes:\", classes)\n",
    "    print(\"📊 Number of classes:\", num_classes)\n",
    "    print(\"🖼️ Total number of images:\", len(full_dataset.samples))\n",
    "    print(\"\\n🔍 Dataset Description:\")\n",
    "    class_counts_dict = Counter(full_dataset.targets)\n",
    "    for i, class_name in enumerate(classes):\n",
    "        num_samples = class_counts_dict[i]\n",
    "        print(f\"  - Class '{class_name}': {num_samples} images\")\n",
    "\n",
    "    # --- Displaying Sample Images ---\n",
    "    num_samples_to_display = 3\n",
    "    fig, axes = plt.subplots(num_classes, num_samples_to_display, figsize=(15, 5 * num_classes))\n",
    "    fig.suptitle(\"Sample Images from Each Class\", fontsize=16)\n",
    "    for i, class_name in enumerate(classes):\n",
    "        indices = np.where(targets == i)[0]\n",
    "        random_indices = np.random.choice(indices, min(num_samples_to_display, len(indices)), replace=False)\n",
    "        for j, img_index in enumerate(random_indices):\n",
    "            image, _ = full_dataset[img_index]\n",
    "            axes[i, j].imshow(image.permute(1, 2, 0))\n",
    "            axes[i, j].set_title(f\"{class_name}\")\n",
    "            axes[i, j].axis('off')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # --- Bar Plot of Class Distribution ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(classes, list(class_counts_dict.values()))\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Number of Images\")\n",
    "    plt.title(\"Distribution of Images per Class\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error loading dataset: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting & Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:24:12.465210Z",
     "iopub.status.busy": "2025-05-12T20:24:12.464871Z",
     "iopub.status.idle": "2025-05-12T20:24:12.499789Z",
     "shell.execute_reply": "2025-05-12T20:24:12.498950Z",
     "shell.execute_reply.started": "2025-05-12T20:24:12.465182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Data Splitting and Handling Imbalance ===\n",
    "train_dataset, val_dataset, test_dataset = None, None, None\n",
    "if UNDERSAMPLE_TURTLE_TORTOISE and 'Turtle_Tortoise' in classes:\n",
    "    turtle_tortoise_index = classes.index('Turtle_Tortoise')\n",
    "    turtle_tortoise_indices = np.where(targets == turtle_tortoise_index)[0]\n",
    "    other_indices = np.where(targets != turtle_tortoise_index)[0]\n",
    "    target_turtle_count = int(np.median(list(class_counts_dict.values())))\n",
    "    undersampled_turtle_indices = np.random.choice(turtle_tortoise_indices, size=target_turtle_count, replace=False)\n",
    "    balanced_indices = np.concatenate([other_indices, undersampled_turtle_indices])\n",
    "    balanced_targets = targets[balanced_indices]\n",
    "\n",
    "    train_idx, val_test_idx = train_test_split(\n",
    "        balanced_indices, test_size=VALIDATION_SPLIT + TEST_SPLIT, stratify=balanced_targets, random_state=SEED\n",
    "    )\n",
    "    val_test_targets = balanced_targets[np.isin(balanced_indices, val_test_idx)]\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        val_test_idx, test_size=TEST_SPLIT / (VALIDATION_SPLIT + TEST_SPLIT), stratify=val_test_targets, random_state=SEED\n",
    "    )\n",
    "    train_dataset = Subset(full_dataset, train_idx)\n",
    "    val_dataset = Subset(full_dataset, val_idx)\n",
    "    test_dataset = Subset(full_dataset, test_idx)\n",
    "    print(f\"Train set size: {len(train_dataset)}\")\n",
    "    print(f\"Val set size: {len(val_dataset)}\")\n",
    "    print(f\"Test set size: {len(test_dataset)}\")\n",
    "else:\n",
    "    train_size = int(len(full_dataset) * (1 - VALIDATION_SPLIT - TEST_SPLIT))\n",
    "    val_size = int(len(full_dataset) * VALIDATION_SPLIT)\n",
    "    test_size = len(full_dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "\n",
    "if train_dataset and val_dataset and test_dataset:\n",
    "    train_dataset.dataset.transform = transform_train\n",
    "    val_dataset.dataset.transform = transform_val\n",
    "    test_dataset.dataset.transform = transform_test\n",
    "\n",
    "    # === Data Loaders ===\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2 * BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=2 * BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "else:\n",
    "    print(\"Error: Datasets not properly created.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Layer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T20:24:12.501345Z",
     "iopub.status.busy": "2025-05-12T20:24:12.500801Z",
     "iopub.status.idle": "2025-05-12T20:24:12.509044Z",
     "shell.execute_reply": "2025-05-12T20:24:12.507965Z",
     "shell.execute_reply.started": "2025-05-12T20:24:12.501314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Quantum Layer Setup ===\n",
    "dev = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def quantum_layer(inputs):\n",
    "    \"\"\"Quantum layer circuit.\"\"\"\n",
    "    for i in range(N_QUBITS):\n",
    "        qml.RX(inputs[i], wires=i)\n",
    "    for i in range(0, N_QUBITS - 1, 2):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    return tuple(qml.expval(qml.PauliZ(i)) for i in range(N_QUBITS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Quantum-Classical Model: HybridQCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T21:16:44.401269Z",
     "iopub.status.busy": "2025-05-12T21:16:44.400968Z",
     "iopub.status.idle": "2025-05-12T21:16:44.410034Z",
     "shell.execute_reply": "2025-05-12T21:16:44.409455Z",
     "shell.execute_reply.started": "2025-05-12T21:16:44.401244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Hybrid Quantum-Classical Model ===\n",
    "class HybridQCNN(nn.Module):\n",
    "    \"\"\"Hybrid quantum-classical convolutional neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, backbone, num_classes, n_qubits=N_QUBITS):\n",
    "        super().__init__()\n",
    "        in_features = self._get_in_features(backbone)\n",
    "        self.feature_extractor = self._remove_classifier(backbone)\n",
    "        self.n_qubits = n_qubits\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features + n_qubits, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "        )\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_in_features(self, model):\n",
    "        if hasattr(model, \"fc\"):\n",
    "            return model.fc.in_features\n",
    "        elif hasattr(model, \"classifier\") and hasattr(model.classifier, \"in_features\"):\n",
    "            return model.classifier.in_features\n",
    "        elif hasattr(model, \"classifier\") and isinstance(model.classifier, nn.Sequential):\n",
    "            for layer in reversed(model.classifier):\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    return layer.in_features\n",
    "        raise ValueError(\"Cannot find the final fully connected layer for feature size.\")\n",
    "\n",
    "    def _remove_classifier(self, model):\n",
    "        if hasattr(model, \"fc\"):\n",
    "            model.fc = nn.Identity()\n",
    "        elif hasattr(model, \"classifier\"):\n",
    "            model.classifier = nn.Identity()\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        q_feats = [quantum_layer(sample[:self.n_qubits].cpu().detach().numpy()) for sample in x]\n",
    "        quantum_features = torch.tensor(q_feats, dtype=torch.float32).to(x.device)\n",
    "        x = torch.cat((x, quantum_features), dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T21:16:46.427620Z",
     "iopub.status.busy": "2025-05-12T21:16:46.427323Z",
     "iopub.status.idle": "2025-05-12T21:16:46.435115Z",
     "shell.execute_reply": "2025-05-12T21:16:46.434579Z",
     "shell.execute_reply.started": "2025-05-12T21:16:46.427598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Classical CNN Model ===\n",
    "class ClassicalCNN(nn.Module):\n",
    "    \"\"\"Classical convolutional neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super().__init__()\n",
    "        in_features = self._get_in_features(backbone)\n",
    "        self.feature_extractor = self._remove_classifier(backbone)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),\n",
    "        )\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_in_features(self, model):\n",
    "        if hasattr(model, \"fc\"):\n",
    "            return model.fc.in_features\n",
    "        elif hasattr(model, \"classifier\") and hasattr(model.classifier, \"in_features\"):\n",
    "            return model.classifier.in_features\n",
    "        elif hasattr(model, \"classifier\") and isinstance(model.classifier, nn.Sequential):\n",
    "            for layer in reversed(model.classifier):\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    return layer.in_features\n",
    "        raise ValueError(\"Cannot find the final fully connected layer for feature size.\")\n",
    "\n",
    "    def _remove_classifier(self, model):\n",
    "        if hasattr(model, \"fc\"):\n",
    "            model.fc = nn.Identity()\n",
    "        elif hasattr(model, \"classifier\"):\n",
    "            model.classifier = nn.Identity()\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T21:16:49.653179Z",
     "iopub.status.busy": "2025-05-12T21:16:49.652503Z",
     "iopub.status.idle": "2025-05-12T21:16:49.665167Z",
     "shell.execute_reply": "2025-05-12T21:16:49.664459Z",
     "shell.execute_reply.started": "2025-05-12T21:16:49.653155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Model Training Function ===\n",
    "def train_model(model, train_loader, val_loader, num_classes):\n",
    "    \"\"\"Trains the given model.\"\"\"\n",
    "    print(f\"\\n🚀 Training {model.__class__.__name__}...\")\n",
    "    model.to(device)\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using DataParallel for multi-GPU training.\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # --- Calculate class weights ---\n",
    "    class_counts_train = Counter()\n",
    "    for _, labels in train_loader:\n",
    "        class_counts_train.update(labels.cpu().numpy())\n",
    "    total_samples_train = sum(class_counts_train.values())\n",
    "    class_weights = torch.ones(num_classes, dtype=torch.float).to(device)\n",
    "    if total_samples_train > 0:\n",
    "        sorted_weights = [total_samples_train / (num_classes * class_counts_train.get(i, 1e-6)) for i in range(num_classes)]\n",
    "        class_weights = torch.tensor(sorted_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=LR_FACTOR, patience=LR_PATIENCE, verbose=True\n",
    "    )\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    all_val_preds, all_val_labels = [], []\n",
    "    all_val_images = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        current_val_preds, current_val_labels = [], []\n",
    "        current_val_images = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images_cpu = images.cpu()\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                current_val_preds.extend(predicted.cpu().numpy())\n",
    "                current_val_labels.extend(labels.cpu().numpy())\n",
    "                current_val_images.extend(images_cpu.numpy())\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        all_val_preds = current_val_preds\n",
    "        all_val_labels = current_val_labels\n",
    "        all_val_images = current_val_images\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{NUM_EPOCHS}, \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\"\n",
    "        )\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    return (\n",
    "        train_losses,\n",
    "        train_accuracies,\n",
    "        val_losses,\n",
    "        val_accuracies,\n",
    "        all_val_preds,\n",
    "        all_val_labels,\n",
    "        all_val_images\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T21:16:58.220930Z",
     "iopub.status.busy": "2025-05-12T21:16:58.220639Z",
     "iopub.status.idle": "2025-05-12T21:16:58.230279Z",
     "shell.execute_reply": "2025-05-12T21:16:58.229520Z",
     "shell.execute_reply.started": "2025-05-12T21:16:58.220909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Model Testing Function ===\n",
    "def test_model(model, test_loader, full_dataset_classes):\n",
    "    \"\"\"Tests the given model.\"\"\"\n",
    "    print(\"\\n🔍 Testing started...\")\n",
    "    model.to(device)\n",
    "\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using DataParallel for multi-GPU testing.\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "    all_test_preds, all_test_labels = [], []\n",
    "    all_test_images = []\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images_cpu = images.cpu()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_test_preds.extend(predicted.cpu().numpy())\n",
    "            all_test_labels.extend(labels.cpu().numpy())\n",
    "            all_test_images.extend(images_cpu.numpy())\n",
    "\n",
    "    test_accuracy = 100 * correct / test_total\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "    print(f\"\\n✅ Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"📉 Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(\"\\n📊 Classification Report:\\n\")\n",
    "    print(classification_report(all_test_labels, all_test_preds, target_names=full_dataset_classes))\n",
    "\n",
    "    cm = confusion_matrix(all_test_labels, all_test_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=full_dataset_classes, yticklabels=full_dataset_classes)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return all_test_preds, all_test_labels, test_accuracy, all_test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison and Metrics Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T21:17:01.610932Z",
     "iopub.status.busy": "2025-05-12T21:17:01.610664Z",
     "iopub.status.idle": "2025-05-12T21:17:01.625031Z",
     "shell.execute_reply": "2025-05-12T21:17:01.624348Z",
     "shell.execute_reply.started": "2025-05-12T21:17:01.610912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Model Comparison Plot ===\n",
    "def plot_model_comparison(all_model_accuracies):\n",
    "    \"\"\"Plots and compares the validation accuracies of all models.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for model_name, accuracies in all_model_accuracies.items():\n",
    "        plt.plot(\n",
    "            range(1, len(accuracies) + 1), accuracies, label=f\"{model_name}\"\n",
    "        )\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Validation Accuracy (%)\")\n",
    "    plt.title(\"Model Comparison: Validation Accuracy Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies, model_name):\n",
    "    \"\"\"Plots training and validation loss and accuracy.\"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'r', label='Training loss')\n",
    "    plt.plot(epochs, val_losses, 'b', label='Validation loss')\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, 'r', label='Training accuracy')\n",
    "    plt.plot(epochs, val_accuracies, 'b', label='Validation accuracy')\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(labels, predictions, classes, model_name):\n",
    "    \"\"\"Plots the confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_classification_report(labels, predictions, classes, model_name):\n",
    "    \"\"\"Prints the classification report.\"\"\"\n",
    "    print(f\"\\n📊 Classification Report for {model_name}:\\n\")\n",
    "    print(classification_report(labels, predictions, target_names=classes))\n",
    "\n",
    "# === Visualization Functions (Updated for Clean Images) ===\n",
    "def denormalize_image(tensor):\n",
    "    \"\"\"Denormalize image tensor using ImageNet stats\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(tensor.device)\n",
    "    return tensor * std + mean\n",
    "\n",
    "def display_predictions(images, true_labels, predicted_labels, classes, num_display=5):\n",
    "    \"\"\"Displays multiple sample images with true and predicted labels in a single horizontal row.\"\"\"\n",
    "    plt.figure(figsize=(15, 3))  # Adjust height as needed\n",
    "\n",
    "    for i in range(min(num_display, len(images))):\n",
    "        # Handle numpy arrays or tensors\n",
    "        if isinstance(images[i], np.ndarray):\n",
    "            img = torch.from_numpy(images[i])\n",
    "        else:\n",
    "            img = images[i]\n",
    "\n",
    "        # Denormalize and convert to HWC format\n",
    "        img = denormalize_image(img)\n",
    "        img = img.cpu().numpy().transpose(1, 2, 0)\n",
    "        img = np.clip(img, 0, 1)  # Ensure valid pixel range\n",
    "\n",
    "        plt.subplot(1, num_display, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True: {classes[true_labels[i]]}\\nPred: {classes[predicted_labels[i]]}\", fontsize=8)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_dataset_images(dataset, classes, num_display=5):\n",
    "    \"\"\"Displays sample images from the dataset in a row.\"\"\"\n",
    "    plt.figure(figsize=(15, 3 * num_display))\n",
    "    for i in range(min(num_display, len(dataset))):\n",
    "        image, label = dataset[i]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        image = denormalize_image(image).cpu().squeeze().numpy().transpose(1, 2, 0)\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "        plt.subplot(1, num_display, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Class: {classes[label]}\", fontsize=10)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T21:17:05.500299Z",
     "iopub.status.busy": "2025-05-12T21:17:05.500026Z",
     "iopub.status.idle": "2025-05-12T21:17:05.505728Z",
     "shell.execute_reply": "2025-05-12T21:17:05.505177Z",
     "shell.execute_reply.started": "2025-05-12T21:17:05.500278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Model Setup ===\n",
    "def get_models(\n",
    "    use_resnet34=True, use_resnet50=True, use_resnet101=True, use_densenet121=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of models based on user-specified flags.\n",
    "    \"\"\"\n",
    "    available_models = {}\n",
    "    if use_resnet34:\n",
    "        available_models[\"ResNet34 (Quantum)\"] = models.resnet34(\n",
    "            weights=ResNet34_Weights.DEFAULT\n",
    "        )\n",
    "        available_models[\"ResNet34 (Classical)\"] = models.resnet34(\n",
    "            weights=ResNet34_Weights.DEFAULT\n",
    "        )\n",
    "    if use_resnet50:\n",
    "        available_models[\"ResNet50 (Quantum)\"] = models.resnet50(\n",
    "            weights=ResNet50_Weights.DEFAULT\n",
    "        )\n",
    "        available_models[\"ResNet50 (Classical)\"] = models.resnet50(\n",
    "            weights=ResNet50_Weights.DEFAULT\n",
    "        )\n",
    "    if use_resnet101:\n",
    "        available_models[\"ResNet101 (Quantum)\"] = models.resnet101(\n",
    "            weights=ResNet101_Weights.DEFAULT\n",
    "        )\n",
    "        available_models[\"ResNet101 (Classical)\"] = models.resnet101(\n",
    "            weights=ResNet101_Weights.DEFAULT\n",
    "        )\n",
    "    if use_densenet121:\n",
    "        available_models[\"DenseNet121 (Quantum)\"] = models.densenet121(\n",
    "            weights=DenseNet121_Weights.DEFAULT\n",
    "        )\n",
    "        available_models[\"DenseNet121 (Classical)\"] = models.densenet121(\n",
    "            weights=DenseNet121_Weights.DEFAULT\n",
    "        )\n",
    "    return available_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T21:17:07.665428Z",
     "iopub.status.busy": "2025-05-12T21:17:07.664831Z",
     "iopub.status.idle": "2025-05-13T03:14:38.858253Z",
     "shell.execute_reply": "2025-05-13T03:14:38.857583Z",
     "shell.execute_reply.started": "2025-05-12T21:17:07.665407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Main Function ===\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the training and testing of selected models.\n",
    "    \"\"\"\n",
    "\n",
    "    all_model_accuracies = {}\n",
    "    all_test_results = {}\n",
    "\n",
    "    # Select models to run\n",
    "    backbones = get_models(\n",
    "        use_resnet34=True, use_resnet50=True, use_resnet101=True, use_densenet121=True\n",
    "    )\n",
    "\n",
    "\n",
    "    for model_name, backbone in backbones.items():\n",
    "        print(f\"\\n--- Training and Testing {model_name} ---\")\n",
    "\n",
    "        model = (\n",
    "            HybridQCNN(backbone, num_classes, n_qubits=N_QUBITS).to(device)\n",
    "            if \"Quantum\" in model_name\n",
    "            else ClassicalCNN(backbone, num_classes).to(device)\n",
    "        )\n",
    "\n",
    "        (\n",
    "            train_losses,\n",
    "            train_accs,\n",
    "            val_losses,\n",
    "            val_accs,\n",
    "            val_preds,\n",
    "            val_labels,\n",
    "            val_images\n",
    "        ) = train_model(model, train_loader, val_loader, num_classes)\n",
    "\n",
    "        all_model_accuracies[model_name] = val_accs\n",
    "\n",
    "        plot_metrics(train_losses, val_losses, train_accs, val_accs, model_name)\n",
    "        plot_confusion_matrix(val_labels, val_preds, full_dataset.classes, f\"{model_name} Validation\")\n",
    "        print_classification_report(\n",
    "            val_labels, val_preds, full_dataset.classes, f\"{model_name} Validation\"\n",
    "        )\n",
    "        display_predictions(val_images, val_labels, val_preds, full_dataset.classes, num_display=5)\n",
    "\n",
    "\n",
    "        test_preds, test_labels, test_acc, test_images = test_model(model, test_loader, full_dataset.classes)\n",
    "        plot_confusion_matrix(\n",
    "            test_labels,test_preds, full_dataset.classes, f\"{model_name} Test\"\n",
    "        )\n",
    "        print_classification_report(\n",
    "            test_labels, test_preds, full_dataset.classes, f\"{model_name} Test\"\n",
    "        )\n",
    "        all_test_results[model_name] = test_acc\n",
    "        display_predictions(test_images, test_labels, test_preds, full_dataset.classes, num_display=5)\n",
    "\n",
    "    plot_model_comparison(all_model_accuracies)\n",
    "\n",
    "    print(\"\\n--- Test Accuracies ---\")\n",
    "    for model_name, test_acc in all_test_results.items():\n",
    "        print(f\"{model_name}: {test_acc:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2442436,
     "sourceId": 5198507,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
